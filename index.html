<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deep Demonstration Tracing: Learning Generalized Imitator for Runtime Imitation from Single Demonstration">
  <meta name="keywords" content="Deep Demonstration Tracing, one-shot imitation learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Deep Demonstration Tracing: Learning Generalizable Imitator Policy for Runtime Imitation from Single Demonstration</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    
    gtag('js', new Date());
    
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<!-- todo huangai qi, yuyan xu, hao ran url -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Deep Demonstration Tracing: Learning Generalizable Imitator Policy for Runtime Imitation from a Single Demonstration</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://xionghuichen.github.io/">Xiong-Hui Chen</a><sup>* 1 2</sup>,</span>
            <span class="author-block">
              <a href="https://www.lamda.nju.edu.cn/yejy/">Junyin Ye</a><sup>* 1 2</sup>,</span>
            <span class="author-block">
              <a href="https://alexfrom0815.github.io/">Hang Zhao</a><sup>* 3 2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.lamda.nju.edu.cn/liyc/">Yi-Chen Li</a><sup>1 2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.lamda.nju.edu.cn/liuxh/">Xu-Hui Liu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/shr1997">Haoran Shi</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/YuyanXu0329">Yu-Yan Xu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.lamda.nju.edu.cn/yezh/">Zhihao Ye</a><sup>1 2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.lamda.nju.edu.cn/yangsh/">Si-Hang Yang</a><sup>1 2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.lamda.nju.edu.cn/yuy">Yang Yu</a><sup>1 2</sup>
            </span>
            <span class="author-block">
              <a href="https://github.com/harticleaq">Anqi Huang</a><sup>4 2</sup>,
            </span>
            <span class="author-block">
              <a href="https://kevinkaixu.net/">Kai Xu</a><sup>3</sup>
              <span class="author-block">
                <a href="https://ai.nju.edu.cn/zhangzongzhang/index.htm">Zongzhang Zhang</a><sup>1</sup>
            </span>

            </span>
          </div>


          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National Key Laboratory for Novel Software Technology, Nanjing University, China
              & School of Artificial Intelligence, Nanjing University, China </span>
            <span class="author-block"><sup>2</sup>Polixr Technologies, </span>
            <span class="author-block"><sup>3</sup>National University of Defense Technology, </span>
            <span class="author-block"><sup>4</sup>Nanjing University of Science and Technology</span>
            <br>
            <span class="author-block"><sup>*</sup>Indicates Equal Contribution</span>
          </div>
    
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=DJdVzxemdA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/xionghuichen/Deep-Demonstration-Tracing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
    
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="1_0" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/1_0.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shelf_place">
          <video poster="" id="shelf_place" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shelf_place.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>
  </div>
</section> -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

<center>
  <img src="./static/images/ddt-res-illustration.png"
           class="interpolation-image"
           style="max-width: 70%;"
           alt="."/>
           <p><b>Illustration of Runtime one-shot imitation learning (OSIL) policies under unforeseen changes in Meta-World tasks.</b>
            The policy in (b) is trained by traditional OSIL. 
            The grasped block may drop by chance due to disturbances that do not exist during demonstration collection. 
         </p>
 </center>

</div>
</div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            One-shot imitation learning (OSIL) is to learn an imitator agent that can execute multiple tasks with only a single demonstration.
            In real-world scenario, the environment is dynamic, e.g., unexpected changes can occur after demonstration. 
            Thus, achieving generalization of the imitator agent is crucial as agents would inevitably face situations unseen in the provided demonstrations.
            While traditional OSIL methods excel in relatively stationary settings, their adaptability to such unforeseen changes, 
            which asking for a higher level of generalization ability for the imitator agents, is limited and rarely discussed (See the following Figure). In this work, we present a new algorithm called <b>D</b>eep <b>D</b>emonstration <b>T</b>racing (DDT).
             In DDT, we propose a demonstration transformer architecture to encourage agents to adaptively trace suitable states in demonstrations. 
             Besides, it integrates OSIL into a meta-reinforcement-learning training paradigm, providing regularization for policies in unexpected situations. 
             We evaluate DDT on a new navigation task suite and robotics tasks, demonstrating its superior performance over existing OSIL methods across all evaluated tasks in dynamic environments with unforeseen changes.
          </p>

        </div>
      </div>
    </div>

  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center>
      <video id="teaser" autoplay loop muted controls style="max-width: 65%;">
        <source src="./static/videos/shelf_place.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay loop muted controls  style="max-width: 65%;">
        <source src="./static/videos/peg_insert.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay loop muted controls  style="max-width: 65%;">
        <source src="./static/videos/sweep.mp4"
                type="video/mp4">
      </video>
    </center>
    <p>
 The first column displays the trajectories generated by DDT for the three tasks: shelf place, peg insert side, and sweep. The second column indicates the closest matched states identified by the attention mechanism, while the third column is the expert trajectories. It can be observed that despite disturbances such as object drops or arm jitter, DDT robustly follows the expert trajectories and completes tasks successfully.
    </p>
    <!-- <h3 class="subtitle has-text-centered">
     
    </h3> -->
    </div>
  </div>
</section>
<section>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Methodology: Deep Demonstration Tracing </h2>
    </div>
    <hr class="rounded">

    <h3 class="title is-4">A. The Demonstration Transformer Architecture for Demonstration Tracing </h3>

      <div class="content has-text-justified">
        <center>
        <img src="./static/images/example-dt.jpg"
                  class="interpolation-image"
                  alt="."/>
        <p> <b>Illustration of Demonstration Transformer Architecture and the motivation example.</b> (a) Illustration of how humans achieve OSIL under unforeseen changes; (b) The demonstration transformer architecture for the actor. 
        \( [s^e_0, \ldots, s^e_i, \ldots, s^e_t] \) denote expert states
        and \( [a^e_0, \ldots, a^e_i, \ldots, a^e_t] \) the expert action list.
        \( s_j \) is the visited state of the actor at timestep \( j \).
          We adopt \( \mathbf{q} \), \( \mathbf{k} \), and \( \mathbf{v} \) to denote the query, key, and value vectors of an attention module. 
          \( N\times \) denotes an \( N \)-layer demo-attention module, which takes the output \( v'' \) of the last layer as the input \( q_j \) of the next layer. 
          Note that the expert-state encoder and the visited state encoder shared the same weights.
        </p>
      </center>
      </div>
    
    <h3 class="title is-4">B. Achieve OSIL via Context-based Meta-RL</h3>

    <div class="content has-text-justified">
       
    <p>
      Conventional OSIL approaches are predominantly based on training with behavior cloning losses, which also fails to guarantee robust decision-making capabilities in unseen states. 
      Drawing inspiration from methodologies that integrate  IL with RL through a stationary imitation reward, we incorporate OSIL into a context-based meta-RL framework. 
      Within this framework, we can utilize the trial-and-error learning mechanism of RL to allow the imitation policy to systematically explore the state space and effectively achieve decision-making proficiency in unseen states.  
    </p>
    
    <center>
      <img src="./static/images/itorl_setting.jpg"
                class="interpolation-image"
                alt="."/>
                <p> <b>Illustration of Training a Runtime One-shot imitator policy via meta-RL.</b> 
      </center>
  </div> 
  </div>
  </section>

<section class="section">
  <div class="container is-max-desktop">

  <div class="columns is-centered has-text-centered">
  <h2 class="title is-3">Experiments</h2>
  </div>
  <hr class="rounded">
  <div class="content has-text-justified">
    <p>In experiments, we focus on answering the following research questions.</p>
    <ul>
      <li><b>RQ1</b>: The one-shot imitation ability of DDT in unseen situations, including <b>unseen demonstrations, unseen environments, and unforeseen changes</b> after demonstration collection.</li>
      <li><b>RQ2</b>: Does demonstration transformer really imitating via tracing the demonstration? </li>
      <li><b>RQ3</b>: Can DDT have potential of performance improvement when scaling up the size of parameters and demonstration data, inspired by the <b>"Scaling Law"</b> in large language models.</li>
      <li><b>RQ4</b>: The performance of DDT when Apply it in Other Challenging Tasks.
    </ul>
  </div>


  <h3 class="title is-4">Environments</h3>
  <div class="content has-text-justified">
    <p>
      We created a challenging benchmark, named Valet Parking Assist in Maze (VPAM), to assess OSIL's performance for unforeseen changes. This navigation benchmark is inspired by a popular and practical real-world application in autonomous driving, called <a href="https://www.youtube.com/watch?v=NLgCXYWqjcg">Valet Parking Assist (VPA)</a>.
      VPAM focuses on navigating diverse, complex mazes without global map information. 
      We also apply DDT in various standard and complex tasks, including meta-world,  <a href="https://jingchengpang.github.io/files/pdf/iros23_o3f.pdf">complex robotics manipulations in clutter</a>, and Reacher and Pusher in MuJoCo to show the robustness of our method in other challenges of OSIL.
    </p>
    <center>
    <img src="./static/images/ddt-exp-ill.png"
    class="interpolation-image"
    alt="."/>
    <p>
      <b>Illustration of Major Experiments in this paper.</b> (A) Illustration of the VPAM, which is a new benchmark for OSIL with unforseen changes.
      The imitation points are provided by our DDT method. (B) Illustration of tasks in Meta-Wolrd. (C) Various Complex tasks of robot manipulation in clutter environments. (a): Grasp the blocked target object (cyan). (b): Stack the objects. (c): Collect the objects scattered over the desk together to the specified area (yellow).
    </p>
  </center>
  </div>
  <h3 class="title is-4">RQ1: One-Shot Imitation Ability in Unseen Situations</h3>
  <div class="container is-max-desktop">

    <div class="columns is-centered">
    
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3"></h2>
          <img src="./static/images/hist.jpg"
                 class="interpolation-image"
                 alt="."/>
          <p>
          <b>Illustration of the imitation policies' performance deployed among different group of settings in VPAM.</b> The black bars denote the standard error among task-group with three seeds.
          Results show that DDT reach the significantly better performance than the baselines from training performance ("Train" Group) to deployment performance whenever with unforseen obstacle ("Unforseen Obstacle" Group) or without obstacle ("Non-Obstacle" Group).
          </p>
        </div>
      </div>
      <!--/ Visual Effects. -->
    
      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3"></h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/pref_curve.JPG"
                 class="interpolation-image"
                 alt="."/>
              <p>
              <b>Illustration of the imitation policies' training performance among different settings.</b> The colored areas denote the standard error among the three seeds.
              DDT displayed a <b>stable and better performance even in the training tasks</b>.  
              We attribute this to the integration of the demonstration transformer architecture. 
              This architecture conferred an additional training efficiency boost by implicitly introducing prior knowledge of how OSIL was achieved, facilitating easier adaptation across various tasks and settings with different complexities.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
  <br />
  <h3 class="title is-4">RQ2: Demonstration-Attention Mechanism for Demonstration Tracing in DDT</h3>
  <p>
   We visualize the agent trajectory in a randomly generated map with unforeseen obstacles and depict attention scores during the decision-making process. 
   The attention scores are the product of the query of current state and the keys associated with demonstration states. 
   It is evident that higher attention values are predominantly concentrated on the diagonal, demonstrating our algorithm's ability to identify which state to follow.
  </p>
<div class="content has-text-justified">
  <center>

  <img src="./static/images/attention.jpg"
  style="max-width: 50%;"
  alt="."/>
  <p>
    <b>Visualizations of DDT in VPAM.</b> (a) A trajectory generated by DDT; (b) The attention score map corresponding to (a). The horizontal and vertical axis represent the agent's trajectory index.
    The deeper color in a row represents the higher attention score.
  </p>
</div>
</center>
  <p>
    Additionally, a corresponding video recording rollouts generated by our DDT method.
  </p>

  <p>
    <center>
    <video id="teaser" autoplay loop muted controls  style="max-width: 70%;">
      <source src="./static/videos/DDT-robot-multi-tasks.mp4"
              type="video/mp4">
    </video>
  </center>
  </p>
<h3 class="title is-4">RQ3: Similar Scaling Law of DDT when Scaling Up in the OSIL Setting</h3>
<div class="content has-text-justified">
  <center>
  <img src="./static/images/scaling-up.png"
  style="max-width: 50%;"
  alt="."/>
  <p>
    Asymptotic performance of DDT under varying demonstration quantities and model parameters, with each unit on the x-axis representing 60 demonstrations or 0.6 million parameters. 
    The x-axis is on a logarithmic scale.  
    <b>Square markers</b> depict the performance of the default DDT parameters.
  </p>
</center>
</div>

<h3 class="title is-4">RQ4: Apply DDT in Other Challenging Tasks.</h3>

<h4 class="title is-5"> Meta-world: Performance under Disturbance</h4>

<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <center>
    <p>
      Results on MetaWorld under Disturbance.
      The video at the start of this project page are rendered from the results in this experiment.
   </p>
    <img src="./static/images/metaworld_table.JPG"
             class="interpolation-image"
             alt="."/>
            </center>
  </div>
</div>

<h4 class="title is-5">Meta-world: Demonstraing with Unseen Heterogeneous Behaviors when Deploying</h4>


<center>
  <img src="./static/images/hero-behave.jpg"
  style="max-width: 100%;"
  alt="."/>
  <p>
    We test and record the generalization performance on three types of unseen heterogeneous demonstrations with all positions of goals without fine-tuning.
  </p>
</center>


<h4 class="title is-5">Complex Manipulation Tasks</h4>

<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <img src="./static/images/cm-res.png"
             class="interpolation-image"
             alt="."/>
             <p>
              Results on Complex Manipulation.
           </p>
  </div>
</div>


</section>

<section class="section">
  <!-- <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Results on Valet Parking Assist in Maze</h2>
        <img src="./static/images/maze_table.JPG"
                 class="interpolation-image"
                 alt="."/>
                 <p>
               </p>
      </div>
    </div> -->


​    


  </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{
      chen2024deep,
      title={Deep Demonstration Tracing: Learning Generalizable Imitator Policy for Runtime Imitation from a Single Demonstration},
      author={Xiong-Hui Chen and Junyin Ye and Hang Zhao and Yi-Chen Li and Xu-Hui Liu and Haoran Shi and Yu-Yan Xu and Zhihao Ye and Si-Hang Yang and Yang Yu and Kai Xu and Zongzhang Zhang and Anqi Huang},
      booktitle={Forty-first International Conference on Machine Learning},
      year={2024},
      url={https://openreview.net/forum?id=DJdVzxemdA}
      }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This webpage template was recycled from <a
              href="https://github.com/nerfies/nerfies.github.io">here</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
